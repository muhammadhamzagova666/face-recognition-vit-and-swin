{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":34595,"sourceType":"datasetVersion","datasetId":26922}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm # Use notebook tqdm for Kaggle\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport argparse\nimport kagglehub # Assuming kagglehub is installed and configured\n\n# --- Configuration ---\n# Using argparse for better hyperparameter management\nparser = argparse.ArgumentParser(description='ViT Training for LFW from Scratch')\nparser.add_argument('--img_size', type=int, default=128, help='Image size')\nparser.add_argument('--patch_size', type=int, default=16, help='Patch size')\nparser.add_argument('--batch_size', type=int, default=64, help='Batch size')\nparser.add_argument('--min_imgs', type=int, default=15, help='Minimum images per person to include') # Increased significantly\nparser.add_argument('--embed_dim', type=int, default=512, help='Embedding dimension')\nparser.add_argument('--depth', type=int, default=6, help='Number of transformer blocks') # Reduced depth slightly\nparser.add_argument('--num_heads', type=int, default=8, help='Number of attention heads')\nparser.add_argument('--mlp_ratio', type=float, default=4.0, help='MLP hidden dimension ratio')\nparser.add_argument('--dropout', type=float, default=0.3, help='Dropout rate') # Increased\nparser.add_argument('--attn_dropout', type=float, default=0.3, help='Attention dropout rate') # Increased\nparser.add_argument('--lr', type=float, default=1e-4, help='Maximum learning rate for OneCycleLR') # Reduced LR\nparser.add_argument('--weight_decay', type=float, default=0.05, help='Weight decay (AdamW)') # Slightly increased WD\nparser.add_argument('--epochs', type=int, default=100, help='Maximum number of epochs')\nparser.add_argument('--patience', type=int, default=15, help='Early stopping patience') # Added early stopping\nparser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')\nparser.add_argument('--data_dir', type=str, default=None, help='Path to dataset directory (will be set by kagglehub)')\n\n# Kaggle notebooks often need this for argparse\n# Use default args if not running from command line with arguments\nargs = parser.parse_args(args=[])\n\n# --- Seed for Reproducibility ---\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed(args.seed)\n\n# --- Download Dataset ---\nprint(\"Downloading LFW dataset...\")\n# Use a temporary variable to store the path returned by kagglehub\ndataset_download_path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\nargs.data_dir = os.path.join(dataset_download_path, \"lfw-deepfunneled/lfw-deepfunneled\")\nprint(f\"Dataset downloaded to: {dataset_download_path}\")\nprint(f\"Using image directory: {args.data_dir}\")\n\n# --- Dataset + Augmentation ---\n# Stronger Augmentation for Training\ntrain_transform = transforms.Compose([\n    transforms.Resize((args.img_size, args.img_size)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(15), # Slightly increased rotation\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1), # Stronger color jitter\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n    transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0), # Added Random Erasing\n])\n\n# Simple Augmentation for Validation\nval_transform = transforms.Compose([\n    transforms.Resize((args.img_size, args.img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n])\n\nclass LFWDataset(Dataset):\n    # Corrected __init__ definition\n    def __init__(self, root_dir, transform=None, min_imgs=2):\n        self.images = []\n        self.labels = []\n        self.transform = transform\n        self.label_map = {}\n        self.class_to_idx = {} # Keep track for mapping back if needed\n        lbl_id = 0\n\n        print(f\"Loading dataset from {root_dir} with min_imgs={min_imgs}\")\n        if not os.path.isdir(root_dir):\n             raise FileNotFoundError(f\"Dataset directory not found: {root_dir}\")\n\n        # Iterate through person folders\n        for person in os.listdir(root_dir):\n            folder = os.path.join(root_dir, person)\n            # Ensure it's a directory\n            if not os.path.isdir(folder): continue\n\n            imgs_in_folder = [f for f in os.listdir(folder) if os.path.splitext(f)[1].lower() == '.jpg']\n            # Filter based on minimum images\n            if len(imgs_in_folder) < min_imgs: continue\n\n            # Assign label ID if not already done\n            if person not in self.label_map:\n                self.label_map[person] = lbl_id\n                self.class_to_idx[lbl_id] = person # Store reverse mapping\n                lbl_id += 1\n\n            # Add images and labels to lists\n            person_label_id = self.label_map[person]\n            for img_name in imgs_in_folder:\n                self.images.append(os.path.join(folder, img_name))\n                self.labels.append(person_label_id)\n\n        self.num_classes = len(self.label_map)\n        print(f\"Found {len(self.images)} images belonging to {self.num_classes} individuals (with >= {min_imgs} images each).\")\n        if self.num_classes == 0:\n            print(f\"Warning: No classes found with min_imgs={min_imgs}. Check data_dir or lower min_imgs.\")\n\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels[idx]\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            # Return a placeholder or skip? For now, let's return None and handle in DataLoader\n            # Or, better, raise an error or return a dummy tensor if your training loop can handle it.\n            # Let's create a dummy tensor matching expected output shape\n            return torch.zeros((3, args.img_size, args.img_size)), -1 # Return invalid label\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n# --- Instantiate + Split Dataset ---\n# Important: Create separate dataset instances for train and val transforms\nfull_train_dataset = LFWDataset(args.data_dir, transform=train_transform, min_imgs=args.min_imgs)\nfull_val_dataset   = LFWDataset(args.data_dir, transform=val_transform,   min_imgs=args.min_imgs)\n\n# Ensure we have data before proceeding\nif len(full_train_dataset) == 0:\n    raise ValueError(\"Dataset is empty. Check `min_imgs` or `data_dir`.\")\n\nNUM_CLASSES = full_train_dataset.num_classes\nprint(f\"Number of classes after filtering: {NUM_CLASSES}\")\n\n# Check if NUM_CLASSES > 0 before splitting\nif NUM_CLASSES == 0:\n     raise ValueError(\"No classes found to train on.\")\nelif NUM_CLASSES == 1:\n    print(\"Warning: Only one class found after filtering. Classification training needs at least 2 classes.\")\n    # Handle this case if needed (e.g., exit or change task)\n    # For now, we'll let it proceed, but train/val split might be trivial\n\n# Generate indices once based on the labels from one of the datasets (they share the same structure)\nindices = list(range(len(full_train_dataset)))\nlabels_for_split = full_train_dataset.labels\n\n# Perform train/validation split\n# Handle cases with very few samples where stratify might fail or be less meaningful\ntest_size = 0.2\nif len(indices) < 5: # Need at least 1 sample per split per class for stratification with 2 classes\n    print(\"Warning: Very few samples, using simple split.\")\n    train_idx, val_idx = train_test_split(indices, test_size=test_size, random_state=args.seed)\nelif NUM_CLASSES > 1:\n    try:\n        train_idx, val_idx = train_test_split(indices,\n                                            test_size=test_size,\n                                            stratify=labels_for_split,\n                                            random_state=args.seed)\n    except ValueError as e:\n         print(f\"Stratified split failed ({e}), falling back to non-stratified split.\")\n         train_idx, val_idx = train_test_split(indices, test_size=test_size, random_state=args.seed)\nelse: # Only one class\n    train_idx, val_idx = train_test_split(indices, test_size=test_size, random_state=args.seed)\n\n\n# Create subset datasets using the generated indices\ntrain_ds = torch.utils.data.Subset(full_train_dataset, train_idx)\nval_ds   = torch.utils.data.Subset(full_val_dataset,   val_idx) # Use val_dataset for correct transforms\n\nprint(f\"Train samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n\n# --- DataLoaders ---\n# Handle potential errors in __getitem__ during loading\ndef collate_fn(batch):\n    batch = list(filter(lambda x: x is not None and x[1] != -1, batch)) # Filter out problematic samples\n    if not batch:\n        return torch.tensor([]), torch.tensor([]) # Return empty tensors if batch is empty\n    return torch.utils.data.dataloader.default_collate(batch)\n\n\ntrain_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True,  num_workers=2, pin_memory=True, collate_fn=collate_fn) # Reduced num_workers for Kaggle\nval_loader   = DataLoader(val_ds,   batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n\n# --- ViT Model Definition (Corrected __init__) ---\nclass PatchEmbedding(nn.Module):\n    # Corrected __init__\n    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        # Projection layer\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n        # Class token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # Positional embedding (learnable)\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.n_patches, embed_dim))\n        # Initialize positional embedding\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        assert H == self.img_size and W == self.img_size, \\\n            f\"Input image size ({H}x{W}) doesn't match model ({self.img_size}x{self.img_size}).\"\n\n        # Project patches: [B, C, H, W] -> [B, E, H/P, W/P]\n        x = self.proj(x)\n        # Flatten and transpose: [B, E, H/P, W/P] -> [B, E, N] -> [B, N, E]\n        x = x.flatten(2).transpose(1, 2)\n        # Prepend class token: [B, 1, E] + [B, N, E] -> [B, N+1, E]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        # Add positional embedding\n        x = x + self.pos_embed\n        return x\n\nclass TransformerEncoder(nn.Module):\n    # Corrected __init__\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, drop=0.1, attn_drop=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=attn_drop, batch_first=True)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        mlp_hidden_dim = int(embed_dim * mlp_ratio)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_hidden_dim),\n            nn.GELU(),\n            nn.Dropout(drop),\n            nn.Linear(mlp_hidden_dim, embed_dim),\n            nn.Dropout(drop),\n        )\n        # Note: Consider adding Stochastic Depth (DropPath) here for stronger regularization if needed\n\n    def forward(self, x):\n        # Self-Attention Block\n        res = x\n        x = self.norm1(x)\n        attn_output, _ = self.attn(x, x, x)\n        x = res + attn_output # Residual connection\n\n        # MLP Block\n        res = x\n        x = self.norm2(x)\n        x = res + self.mlp(x) # Residual connection\n        return x\n\nclass ViT(nn.Module):\n     # Corrected __init__\n    def __init__(self,\n                 img_size=128,\n                 patch_size=16,\n                 in_chans=3,\n                 num_classes=1000,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.0,\n                 drop_rate=0.1,\n                 attn_drop_rate=0.1):\n        super().__init__()\n        self.num_classes = num_classes\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n        # Transformer Blocks\n        self.blocks = nn.ModuleList([\n            TransformerEncoder(embed_dim, num_heads, mlp_ratio, drop=drop_rate, attn_drop=attn_drop_rate)\n            for _ in range(depth)])\n        # Final normalization and classifier head\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity() # Handle num_classes=0 case\n\n        # Weight initialization (important for training from scratch)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward(self, x):\n        # Patch embedding\n        x = self.patch_embed(x) # [B, N+1, E]\n        # Pass through transformer blocks\n        for blk in self.blocks:\n            x = blk(x)\n        # Final normalization (on CLS token)\n        x = self.norm(x[:, 0]) # [B, E]\n        # Classification head\n        x = self.head(x) # [B, num_classes]\n        return x\n\n# --- Setup Training ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Ensure NUM_CLASSES is valid before creating the model\nif NUM_CLASSES <= 0:\n    raise ValueError(f\"Invalid number of classes: {NUM_CLASSES}. Check dataset filtering.\")\n\n\nmodel = ViT(img_size=args.img_size,\n            patch_size=args.patch_size,\n            in_chans=3,\n            num_classes=NUM_CLASSES,\n            embed_dim=args.embed_dim,\n            depth=args.depth,\n            num_heads=args.num_heads,\n            mlp_ratio=args.mlp_ratio,\n            drop_rate=args.dropout,\n            attn_drop_rate=args.attn_dropout).to(device)\n\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Loss Function with Label Smoothing\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# Optimizer (AdamW recommended for transformers)\noptimizer = torch.optim.AdamW(model.parameters(),\n                              lr=args.lr, # This is max_lr for OneCycleLR\n                              weight_decay=args.weight_decay)\n\n# Scheduler: OneCycleLR is good for faster convergence but can sometimes overshoot.\n# CosineAnnealingLR might be gentler if OneCycleLR causes instability.\nsteps_per_epoch = len(train_loader)\nif steps_per_epoch == 0:\n     raise ValueError(\"Train loader is empty. Cannot determine steps per epoch.\")\n\nscheduler = OneCycleLR(optimizer,\n                       max_lr=args.lr,\n                       steps_per_epoch=steps_per_epoch,\n                       epochs=args.epochs,\n                       pct_start=0.1, # Slightly longer warmup\n                       anneal_strategy='cos')\n\n# Gradient scaler for mixed precision (optional but recommended for speed/memory on modern GPUs)\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\n# --- Training + Validation Loop ---\nbest_val_acc = 0.0\nbest_epoch = 0\nepochs_no_improve = 0\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []} # Track history\n\nprint(f\"Starting training for {args.epochs} epochs...\")\n\nfor epoch in range(1, args.epochs + 1):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    t0 = time.time()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{args.epochs} [Train]\")\n\n    for imgs, labels in pbar:\n        # Skip batch if it became empty after filtering in collate_fn\n        if imgs.nelement() == 0:\n            continue\n\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        # Mixed precision training\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n\n        # Scale loss and backpropagate\n        scaler.scale(loss).backward()\n\n        # Optional: Gradient Clipping (unscale first if using scaler)\n        scaler.unscale_(optimizer) # Unscale before clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step() # Step OneCycleLR *every iteration*\n\n        running_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n        pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n\n    # Calculate epoch stats (handle division by zero if total is 0)\n    train_loss = running_loss / total if total > 0 else 0\n    train_acc  = correct / total if total > 0 else 0\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    t1 = time.time()\n\n    # --- Validation ---\n    model.eval()\n    val_running_loss, val_corr, val_tot = 0.0, 0, 0\n    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{args.epochs} [Val]\")\n\n    with torch.no_grad():\n        for imgs, labels in val_pbar:\n            # Skip batch if it became empty after filtering in collate_fn\n            if imgs.nelement() == 0:\n                 continue\n\n            imgs, labels = imgs.to(device), labels.to(device)\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                logits = model(imgs)\n                loss   = criterion(logits, labels) # Calculate val loss too\n\n            val_running_loss += loss.item() * imgs.size(0)\n            preds  = logits.argmax(dim=1)\n            val_corr += (preds == labels).sum().item()\n            val_tot  += imgs.size(0)\n            val_pbar.set_postfix({'val_acc': val_corr / val_tot if val_tot > 0 else 0})\n\n\n    val_loss = val_running_loss / val_tot if val_tot > 0 else 0\n    val_acc  = val_corr / val_tot if val_tot > 0 else 0\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n\n    print(f\"Epoch {epoch:03d} | \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n          f\"LR: {scheduler.get_last_lr()[0]:.6f} | Time: {t1-t0:.1f}s\")\n\n    # --- Save Best Model & Early Stopping ---\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_epoch = epoch\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_acc': best_val_acc,\n            'num_classes': NUM_CLASSES,\n            'class_to_idx': full_train_dataset.class_to_idx, # Save mapping\n            'args': args # Save config\n        }, \"best_vit_lfw_scratch.pth\")\n        print(f\"*** Best validation accuracy improved to {best_val_acc:.4f}. Model saved. ***\")\n        epochs_no_improve = 0 # Reset counter\n    else:\n        epochs_no_improve += 1\n        print(f\"Validation accuracy did not improve ({val_acc:.4f} vs best {best_val_acc:.4f}). {epochs_no_improve}/{args.patience}\")\n\n    if epochs_no_improve >= args.patience:\n        print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n        break\n\nprint(f\"\\nTraining finished.\")\nprint(f\"Best validation accuracy: {best_val_acc:.4f} achieved at epoch {best_epoch}\")\nprint(\"Best model weights saved to best_vit_lfw_scratch.pth\")\n\n# You can plot the history later if needed\n# import matplotlib.pyplot as plt\n# plt.figure(figsize=(12, 5))\n# plt.subplot(1, 2, 1)\n# plt.plot(history['train_loss'], label='Train Loss')\n# plt.plot(history['val_loss'], label='Val Loss')\n# plt.legend()\n# plt.title('Loss Curve')\n# plt.subplot(1, 2, 2)\n# plt.plot(history['train_acc'], label='Train Acc')\n# plt.plot(history['val_acc'], label='Val Acc')\n# plt.legend()\n# plt.title('Accuracy Curve')\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}