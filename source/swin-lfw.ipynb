{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":34595,"sourceType":"datasetVersion","datasetId":26922}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-30T11:28:05.093350Z","iopub.execute_input":"2025-04-30T11:28:05.093503Z","iopub.status.idle":"2025-04-30T11:28:07.122854Z","shell.execute_reply.started":"2025-04-30T11:28:05.093489Z","shell.execute_reply":"2025-04-30T11:28:07.122280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.notebook import tqdm # Use notebook tqdm for Kaggle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nimport argparse\nimport kagglehub # Assuming kagglehub is installed\nfrom timm import create_model # For Swin Transformer\n\n# --- Configuration via argparse (Consistent with ViT script) ---\nparser = argparse.ArgumentParser(description='Swin Transformer Training for LFW from Scratch')\n# Model/Data Params\nparser.add_argument('--model_name', type=str, default='swin_tiny_patch4_window7_224', help='Swin model variant from timm')\nparser.add_argument('--img_size', type=int, default=224, help='Image size (Swin models often use 224)')\nparser.add_argument('--min_imgs', type=int, default=15, help='Minimum images per person to include (Crucial!)')\nparser.add_argument('--batch_size', type=int, default=32, help='Batch size (Reduce if OOM with Swin-T)') # Adjusted default BS\nparser.add_argument('--data_dir', type=str, default=None, help='Path to dataset directory (will be set by kagglehub)')\n# Training Params\nparser.add_argument('--lr', type=float, default=1e-4, help='Maximum learning rate for OneCycleLR') # Consistent LR\nparser.add_argument('--weight_decay', type=float, default=0.05, help='Weight decay (AdamW)') # Consistent WD\nparser.add_argument('--epochs', type=int, default=100, help='Maximum number of epochs') # Consistent Epochs\nparser.add_argument('--patience', type=int, default=15, help='Early stopping patience') # Consistent Patience\nparser.add_argument('--label_smoothing', type=float, default=0.1, help='Label smoothing factor')\nparser.add_argument('--drop_path_rate', type=float, default=0.1, help='Stochastic depth rate for Swin') # Added for Swin\nparser.add_argument('--seed', type=int, default=42, help='Random seed for reproducibility')\n\n# Kaggle notebooks often need this for argparse\n# Use default args if not running from command line with arguments\nargs = parser.parse_args(args=[]) # Use [] for default args in notebook\n\n# --- Seed for Reproducibility ---\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True # Ensure deterministic behavior\n        torch.backends.cudnn.benchmark = False # Disable benchmark for determinism\n\nset_seed(args.seed)\n\n# --- Download Dataset Path ---\nprint(\"Getting LFW dataset path...\")\n# Avoid re-downloading if path is already known or dataset exists\n# Assuming the structure matches ViT setup now\ntry:\n    dataset_download_path = kagglehub.dataset_download(\"jessicali9530/lfw-dataset\")\n    args.data_dir = os.path.join(dataset_download_path, \"lfw-deepfunneled/lfw-deepfunneled\")\n    print(f\"Using dataset directory: {args.data_dir}\")\n    if not os.path.isdir(args.data_dir):\n        raise FileNotFoundError(\"Dataset directory not found after kagglehub download.\")\nexcept Exception as e:\n    # Fallback or direct path if kagglehub fails or path is known\n    # Example: args.data_dir = '/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled'\n    print(f\"Warning: kagglehub download check failed ({e}). Attempting to use default path.\")\n    args.data_dir = '/kaggle/input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled' # Adjust if necessary\n    if not os.path.isdir(args.data_dir):\n         raise FileNotFoundError(f\"Dataset directory not found at specified path: {args.data_dir}\")\n    print(f\"Using manually specified dataset directory: {args.data_dir}\")\n\n\n# --- Dataset Class (Using the same LFWDataset as ViT) ---\nclass LFWDataset(Dataset):\n    def __init__(self, root_dir, transform=None, min_imgs=2, img_size=224): # Default img_size\n        self.images = []\n        self.labels = []\n        self.transform = transform\n        self.label_map = {}\n        self.class_to_idx = {}\n        self.img_size = img_size\n        lbl_id = 0\n        # Actual data loading done by _load_data\n\n    def _load_data(self, root_dir, min_imgs):\n        self.images = []\n        self.labels = []\n        self.label_map = {}\n        self.class_to_idx = {}\n        lbl_id = 0\n        print(f\"Loading dataset structure from {root_dir} with min_imgs={min_imgs}\")\n        if not os.path.isdir(root_dir):\n             raise FileNotFoundError(f\"Dataset directory not found: {root_dir}\")\n\n        for person in os.listdir(root_dir):\n            folder = os.path.join(root_dir, person)\n            if not os.path.isdir(folder): continue\n            imgs_in_folder = [f for f in os.listdir(folder) if os.path.splitext(f)[1].lower() == '.jpg']\n            if len(imgs_in_folder) < min_imgs: continue\n\n            if person not in self.label_map:\n                self.label_map[person] = lbl_id\n                self.class_to_idx[lbl_id] = person\n                lbl_id += 1\n\n            person_label_id = self.label_map[person]\n            for img_name in imgs_in_folder:\n                self.images.append(os.path.join(folder, img_name))\n                self.labels.append(person_label_id)\n\n        self.num_classes = len(self.label_map)\n        print(f\"Found {len(self.images)} images belonging to {self.num_classes} individuals (with >= {min_imgs} images each).\")\n        if self.num_classes == 0:\n            print(f\"Warning: No classes found with min_imgs={min_imgs}. Check data_dir or lower min_imgs.\")\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        label = self.labels[idx]\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            return torch.zeros((3, self.img_size, self.img_size)), -1 # Use correct size\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n\n# --- Augmentations (Consistent with ViT, adjusted for img_size=224) ---\n# Using normalization to [-1, 1] like in ViT script\ntrain_transform = transforms.Compose([\n    transforms.Resize((args.img_size, args.img_size)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]), # Normalizing to [-1, 1]\n    transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3), value=0),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((args.img_size, args.img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]), # Normalizing to [-1, 1]\n])\n\n# --- Instantiate + Split Dataset (Filtered!) ---\nfull_train_dataset = LFWDataset(args.data_dir, transform=train_transform, img_size=args.img_size)\nfull_train_dataset._load_data(args.data_dir, args.min_imgs) # Load with filtering\n\nfull_val_dataset = LFWDataset(args.data_dir, transform=val_transform, img_size=args.img_size)\nfull_val_dataset._load_data(args.data_dir, args.min_imgs) # Load structure for split\n\nif len(full_train_dataset) == 0:\n    raise ValueError(\"Dataset is empty after filtering. Check `min_imgs` or `data_dir`.\")\n\nNUM_CLASSES = full_train_dataset.num_classes\nprint(f\"Number of classes after filtering: {NUM_CLASSES}\")\nif NUM_CLASSES <= 1:\n     raise ValueError(f\"Need at least 2 classes for training, found {NUM_CLASSES}. Adjust min_imgs.\")\n\n# Stratified Split (Consistent with ViT)\nindices = list(range(len(full_train_dataset)))\nlabels_for_split = full_train_dataset.labels\ntest_size = 0.2\n\ntry:\n    train_idx, val_idx = train_test_split(indices,\n                                        test_size=test_size,\n                                        stratify=labels_for_split,\n                                        random_state=args.seed)\nexcept ValueError as e:\n     print(f\"Stratified split failed ({e}), falling back to non-stratified split.\")\n     train_idx, val_idx = train_test_split(indices, test_size=test_size, random_state=args.seed)\n\ntrain_ds = torch.utils.data.Subset(full_train_dataset, train_idx)\n# Use the val_dataset instance for validation subset to ensure correct transform\nval_ds = torch.utils.data.Subset(full_val_dataset, val_idx)\n\nprint(f\"Train samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n\n# --- DataLoaders ---\ndef collate_fn(batch):\n    batch = list(filter(lambda x: x is not None and x[1] != -1, batch))\n    if not batch: return torch.tensor([]), torch.tensor([])\n    return torch.utils.data.dataloader.default_collate(batch)\n\ntrain_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_fn)\n\n# --- Swin Transformer Model (From Scratch) ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nprint(f\"Creating Swin model: {args.model_name} from scratch...\")\nmodel = create_model(\n    args.model_name,\n    pretrained=False, # IMPORTANT: Train from scratch\n    num_classes=NUM_CLASSES,\n    drop_path_rate=args.drop_path_rate # Stochastic depth regularization\n)\nmodel.to(device)\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# --- Setup Training (Consistent with ViT) ---\n# Loss Function with Label Smoothing\ncriterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)\n\n# Optimizer (AdamW)\noptimizer = optim.AdamW(model.parameters(),\n                          lr=args.lr, # max_lr for OneCycleLR\n                          weight_decay=args.weight_decay)\n\n# Scheduler (OneCycleLR)\nsteps_per_epoch = len(train_loader)\nif steps_per_epoch == 0:\n     raise ValueError(\"Train loader is empty. Cannot determine steps per epoch.\")\n\nscheduler = OneCycleLR(optimizer,\n                       max_lr=args.lr,\n                       steps_per_epoch=steps_per_epoch,\n                       epochs=args.epochs,\n                       pct_start=0.1, # Warmup 10% like ViT setup\n                       anneal_strategy='cos')\n\n# Gradient scaler for mixed precision (Recommended)\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\n# --- Training + Validation Loop (Adapted from ViT) ---\nbest_val_acc = 0.0\nbest_epoch = 0\nepochs_no_improve = 0\nhistory = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\nprint(f\"Starting training for {args.epochs} epochs...\")\n\nfor epoch in range(1, args.epochs + 1):\n    # --- Training Phase ---\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    t0 = time.time()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{args.epochs} [Train]\")\n\n    for imgs, labels in pbar:\n        if imgs.nelement() == 0: continue # Skip empty batch\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n\n        scaler.scale(loss).backward()\n\n        # Gradient Clipping (Consistent with ViT)\n        scaler.unscale_(optimizer) # Required before clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step() # Step OneCycleLR every iteration\n\n        running_loss += loss.item() * imgs.size(0)\n        preds = logits.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += imgs.size(0)\n\n        pbar.set_postfix({'loss': loss.item(), 'lr': scheduler.get_last_lr()[0]})\n\n    train_loss = running_loss / total if total > 0 else 0\n    train_acc = correct / total if total > 0 else 0\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    t1 = time.time()\n\n    # --- Validation Phase ---\n    model.eval()\n    val_running_loss, val_corr, val_tot = 0.0, 0, 0\n    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch}/{args.epochs} [Val]\")\n\n    with torch.no_grad():\n        for imgs, labels in val_pbar:\n            if imgs.nelement() == 0: continue # Skip empty batch\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n                logits = model(imgs)\n                loss = criterion(logits, labels)\n\n            val_running_loss += loss.item() * imgs.size(0)\n            preds = logits.argmax(dim=1)\n            val_corr += (preds == labels).sum().item()\n            val_tot += imgs.size(0)\n            val_pbar.set_postfix({'val_acc': val_corr / val_tot if val_tot > 0 else 0})\n\n    val_loss = val_running_loss / val_tot if val_tot > 0 else 0\n    val_acc = val_corr / val_tot if val_tot > 0 else 0\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)\n\n    print(f\"Epoch {epoch:03d} | \"\n          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n          f\"LR: {scheduler.get_last_lr()[0]:.6f} | Time: {t1-t0:.1f}s\")\n\n    # --- Save Best Model & Early Stopping (Consistent with ViT) ---\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_epoch = epoch\n        # Make sure the path exists if needed, e.g., /kaggle/working/\n        os.makedirs(\"/kaggle/working/\", exist_ok=True)\n        save_path = \"/kaggle/working/best_swin_lfw_scratch.pth\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'best_val_acc': best_val_acc,\n            'num_classes': NUM_CLASSES,\n            'class_to_idx': full_train_dataset.class_to_idx, # Save mapping\n            'args': args, # Save config\n            'history': history # Save history for plotting\n        }, save_path)\n        print(f\"*** Best validation accuracy improved to {best_val_acc:.4f}. Model saved to {save_path} ***\")\n        epochs_no_improve = 0\n    else:\n        epochs_no_improve += 1\n        print(f\"Validation accuracy did not improve ({val_acc:.4f} vs best {best_val_acc:.4f}). {epochs_no_improve}/{args.patience}\")\n\n    if epochs_no_improve >= args.patience:\n        print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n        break\n\nprint(f\"\\nTraining finished.\")\nprint(f\"Best validation accuracy: {best_val_acc:.4f} achieved at epoch {best_epoch}\")\nprint(\"Best model weights saved to /kaggle/working/best_swin_lfw_scratch.pth\")\n\n# Optional: Save final model state as well\ntorch.save(model.state_dict(), \"/kaggle/working/final_swin_lfw_scratch.pth\")\n\n# Optional: Plot history if needed immediately (or load from checkpoint later)\n# import matplotlib.pyplot as plt\n# plt.figure(figsize=(12, 5))\n# plt.subplot(1, 2, 1); plt.plot(history['train_loss'], label='Train Loss'); plt.plot(history['val_loss'], label='Val Loss'); plt.legend(); plt.title('Loss')\n# plt.subplot(1, 2, 2); plt.plot(history['train_acc'], label='Train Acc'); plt.plot(history['val_acc'], label='Val Acc'); plt.legend(); plt.title('Accuracy')\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T11:28:07.123973Z","iopub.execute_input":"2025-04-30T11:28:07.124418Z","iopub.status.idle":"2025-04-30T11:29:15.737088Z","shell.execute_reply.started":"2025-04-30T11:28:07.124392Z","shell.execute_reply":"2025-04-30T11:29:15.735971Z"}},"outputs":[],"execution_count":null}]}